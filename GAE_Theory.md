#### 理论推导
1. 优势函数  
$A(a_t,s_t)=Q(a_t,s_t)-V(s_t)$  

2. 对 $Q$ 进行采样和估计  
$Q(a_t,s_t)=E[r_t+ \gamma V(s_{t+1})]$  

3. 代入优势函数，得到1步优势估计器  
$A(a_t,s_t)=E[r_t+ \gamma V(s_{t+1})-V(s_t)]$  

4. 类似的，对 $V$ 也可以进行采样和估计  
$V(s_{t+1})=E[r_{t+1}+ \gamma V(s_{t+2})]$  

5. 代入1步优势估计器，得到2步优势估计器  
$A(a_t,s_t)=E[r_t+ \gamma V(s_{t+1}) + \gamma^2 V(s_{t+1}) - V(s_t)]$  

6. 以此类推，可以得到无穷步优势估计器  
$A(a_t,s_t)=E[r_t+ \gamma V(s_{t+1}) + \gamma^2 V(s_{t+1}) +  \gamma^3 V(s_{t+2}) + ... + - V(s_t)]$  
这就是蒙特卡洛估计：从 $t$ 时刻开始，收集直至回合结束的所有奖励，因此采样的是每条轨迹的真实回报（**偏差小**）；但由于轨迹链长、动作多，因此随机性大（**方差大**）  

7. 相反，1步优势估计器因为估计不准，所以**偏差大**，但是**方差小**  

8. 为了平衡估计方差和偏差，提出GAE，通过指数平均综合不同步的优势估计器。令 $\delta_t=r_t+ \gamma V(s_{t+1})-V(s_t)$ ，可以得到：  
1步优势估计器 $A_t^{(1)}=\delta_t$  
2步优势估计器 $A_t^{(2)}=\delta_t+\gamma \delta_{t+1}$  
...  
k步优势估计器 $A_t^{(k)}=\sum_{l=0}^{k-1} \gamma^l \delta_{t+l}$  

9. 对上述优势估计器进行指数平均  
$A^{GAE}(a_t,s_t)=(1-\lambda)(A_t^{(1)}+\lambda A_t^{(2)} + \lambda^2 A_t^{(3)}+...)$  
$=\sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$  